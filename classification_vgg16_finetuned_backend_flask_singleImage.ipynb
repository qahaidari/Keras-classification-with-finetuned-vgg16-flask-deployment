{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification-vgg16-finetuned-backend-flask-singleImage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNoX0yW3iDQVjNSxoGB5LwT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qahaidari/Keras-classification-with-finetuned-vgg16-flask-deployment/blob/main/classification_vgg16_finetuned_backend_flask_singleImage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozSSrIrHMD5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53c5d32-8a25-42a8-b9fb-1b68b948072c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIGk5xMhLlPM"
      },
      "source": [
        "In this project, we want to use the [fine-tuned VGG16 model](https://github.com/qahaidari/Keras-classification-with-finetuned-vgg16-flask-deployment/blob/main/DogsvsCats_vgg16_finetuned.ipynb) to do predictions on images of cats and dogs. The VGG16 model is already fine-tuned to predict on two classes of cats and dogs. Then the model will be deployed on Flask. A Flask web service for the backend of the project and a Flask web application for the frontend of the project will be implemented. In the frontend, the user can upload an image of a cat or a dog and send it to the backend web service, where the fine-tuned VGG16 model will receive the image, do a prediction and return the result to the user. Because the aim is to use VGG16 neural network to predict on images of cats and dogs, the Flask services for building frontend and backend parts of this application are implemented on Google Colab so that we can use the Colab GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expLNHagL9dJ"
      },
      "source": [
        "For running Flask on Colab, flask-ngrok should be installed; [tutorial](https://medium.com/@kshitijvijay271199/flask-on-google-colab-f6525986797b)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UeCzK23TBsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2125ae4-05c1-471a-c6e7-c809798fde7b"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZrhiCUGME-t"
      },
      "source": [
        "Below is the implementation of the backend for a simple application where user sends some data from the client side as a json file and the backend receives it and sends back a response after processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOVeSDz22Yya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9ab789-7f18-4003-c127-8e8347150bb9"
      },
      "source": [
        "# a simple backend with flask for returning what the user types on frontend\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import request\n",
        "from flask import jsonify\n",
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "@app.route('/',methods=['POST'])\n",
        "def hello():\n",
        "    message = request.get_json(force=True)\n",
        "    name = message['name']\n",
        "    response = {\n",
        "        'greeting': 'Hello, ' + name + '!'\n",
        "    }\n",
        "    return jsonify(response)\n",
        "\n",
        "app.run()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://006e48e16b29.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnDzXpK-DuRS"
      },
      "source": [
        "Now let's develop a backend web service where predictions of images of cats and dogs are implemented using a fine-tuned VGG16 model. The fine-tuned VGG16 model is available in the same directory in the form of a h5 file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgI_8NdFD_2V"
      },
      "source": [
        "import base64\n",
        "import numpy as np\n",
        "import io\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
        "from flask import request\n",
        "from flask import jsonify\n",
        "from flask import Flask\n",
        "from flask_ngrok import run_with_ngrok"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZBk9ijvFJJx"
      },
      "source": [
        "app = Flask(__name__)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-B9bm2wcLx-"
      },
      "source": [
        "We define a function to load our VGG16 fine-tuned h5 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa5uq95VHjWU"
      },
      "source": [
        "# function to load h5 file of the VGG16 fine-tuned model that we had saved already\n",
        "def get_model():\n",
        "    global model\n",
        "    model = load_model('/content/drive/MyDrive/dogs-vs-cats/VGG16_cats_and_dogs.h5')\n",
        "    print(\" * Model loaded!\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBqyUmdpGBjL"
      },
      "source": [
        "# this function preprocesses the image so that it can be passed to our model\n",
        "def preprocess_image(image, target_size):\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "    image = image.resize(target_size)\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    return image"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pyFyQb3Hg-T",
        "outputId": "53a40e60-9a28-4a72-cddd-75834c66e7ae"
      },
      "source": [
        "print(\" * Loading Keras model...\")\n",
        "get_model()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Loading Keras model...\n",
            " * Model loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrwJscDgH3aR",
        "outputId": "511223fe-b539-4e24-9aaf-39580c471219"
      },
      "source": [
        "run_with_ngrok(app)\n",
        "@app.route(\"/\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    message = request.get_json(force=True)\n",
        "    encoded = message['image']\n",
        "    decoded = base64.b64decode(encoded)\n",
        "    image = Image.open(io.BytesIO(decoded))\n",
        "    processed_image = preprocess_image(image, target_size=(224, 224))\n",
        "\n",
        "    prediction = model.predict(processed_image).tolist()\n",
        "\n",
        "    response = {\n",
        "        'prediction': {\n",
        "            'cat': prediction[0][0],\n",
        "            'dog': prediction[0][1]\n",
        "        }\n",
        "    }\n",
        "    return jsonify(response)\n",
        "\n",
        "app.run()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://ba8cf2cc7cdf.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [13/Apr/2021 22:21:42] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Apr/2021 22:28:38] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Apr/2021 22:28:49] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}